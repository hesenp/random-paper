\documentclass[12pt,letterpaper]{article}

\usepackage{url}
\usepackage{natbib}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsthm}

\newtheorem{thm}{Theorem}

\begin{document}

\bibliographystyle{plain}

\title{Generalized Distand Association} 
\author{Hesen Peng \and Tianwei Yu}
\date{Draft: \today{}} 
\maketitle{}

Given two random vectors $X$ and $Y$, we are interested in testing
their probablistic association given $n$ pairs of independent and
identically distributed random samples $\{(X_i, Y_i)\}_{i=1}^n$.
\cite{cite:10.1371/journal.pone.0124620} proposed Mean Distance
Association (MeDiA), a set of probablistic association statistics as
functions of observation distances. The theoretical foundation of
MeDiA relies upon the result below: 

\begin{thm}
  \label{thm:1} 
  (from \cite{cite:10.1371/journal.pone.0124620}) Denote the distance
  between two independent random samples from $(X,Y)$ as $d_{XY}$, and
  the distance between two independent random samples from
  $(\hat{X},\hat{Y})$ as $d_{\hat{X}\hat{Y}}$. Then we have
  \begin{displaymath}
    E(d_{XY}) \le E(d_{\hat{X}\hat{Y}})
  \end{displaymath}
\end{thm}

In this paper, we would like to expand the theory above to general
functions on the observation graph. The generalized mean distance
would encompass a number of existing methods, like mutual
information. Besides, the generalized mean distance naturally leads to
the construction of several other probabilistic association
statistics.

\begin{thm}
  \label{thm:2}
  (univariate $g$-transformation) Using the same notation, denote a
  monotonically increasing continuously differentiable function
  $g(\cdot)$. Denote the $g$-transformed distance as
  \begin{eqnarray*}
    \tilde{d}_{XY} &=& g(d_{XY})\\
    \tilde{d}_{\hat{X}\hat{Y}} &=& g(d_{\hat{X}\hat{Y}})
  \end{eqnarray*}
  Then we have:
  \begin{displaymath}
    E(\tilde{d}_{XY}) \le E(\tilde{d}_{\hat{X}\hat{Y}})
  \end{displaymath}
  and the average of the transformed distances $\tilde{d}$ follow
  asymptotic normal distribution.
\end{thm}

\begin{proof}
  The proof follows directly from delta method. More specifically, for
  given $d_{\hat{X}\hat{Y}}$ and $d_{XY}$, there exists $d_{XY}'$, such that: 
  \begin{eqnarray*}
    \tilde{d}_{\hat{X}\hat{Y}} &=& g(d_{\hat{X}\hat{Y}})\\
                               &=& g
                                   \left[
                                   (d_{\hat{X}\hat{Y}} - d_{XY}) + d_{XY}
                                   \right] \\
                               &=& g(d_{XY}) + g'(d_{XY}') (d_{\hat{X}\hat{Y}}' - d_{XY})\\
                               &=& \tilde{d}_{XY} + g'(d_{XY}') (d_{\hat{X}\hat{Y}} - d_{XY})
  \end{eqnarray*}
  Following Theorem \ref{thm:1}, taking expectation on both sides, and
  realizing that $g'(\cdot) \ge 0$, we conclude the proof.
\end{proof}

Following Theorem \ref{thm:2}, we can see that distance based mutual
information statistic $MI = \sum \log(d_{ij})$ actually falls into the
generalized mean distance family.

However, it would be helpful to realize that Theorem 2 has not yet
encompass functions on the observation graph that give different
weigths depending on the value. We would make this up with the results below: 

\begin{thm}
  \label{thm:3}
  (Multivariate $f$-transformation) Using the same notation as above,
  $n$-variate function $f$ is continuous and monotonically increasing
  on every dimension of input. Define 
  \begin{eqnarray*}
    \bar{d}_{XY} &=& g(d^{XY}_{i1},\ldots,d^{XY}_{in})\\ 
    \bar{d}_{\hat{X}\hat{Y}} &=& g(d^{\hat{X}\hat{Y}}_{i1},\ldots,d^{\hat{X}\hat{Y}}_{in})\\
  \end{eqnarray*}
  Then we have:
  \begin{displaymath}
    E(\bar{d}_{XY}) \le{} E(\bar{d}_{\hat{X}\hat{Y}})
  \end{displaymath}
\end{thm}



\bibliography{gen-media}

\end{document}
